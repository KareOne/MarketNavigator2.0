from config.config import get_db_connection, get_global_bot
from utils.logger import bot_logger
import time
from core.bot.linkdeen_bot import LinkedinBot
import random
import pymysql
import os
from utils.exceptions import LoginError
import base64
from services.socket_handlers import get_socket_handlers
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException


def get_page(request):
    """
    Ø§ÛŒÙ† endpoint ÙÙ‚Ø· Ù†Ø§Ù… Page Ø±Ø§ Ø¯Ø± ØµÙ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    Worker Ø®ÙˆØ¯Ø´ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """
    try:
        data = request.get_json()
        page_name = data.get("page_name")
        user_id = data.get("user_id")  # Ø§Ø®ØªÛŒØ§Ø±ÛŒ - Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ socket events
        
        if not page_name:
            return {"status": "error", "message": "Ù†Ø§Ù… Page Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª"}, 400
        
        # Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
        page_name = page_name.strip()
        
        # Ú†Ú© Ú©Ø±Ø¯Ù† ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù†
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute(
            "SELECT id, status FROM pages_queue WHERE page_name = %s AND status IN ('pending', 'processing') LIMIT 1",
            (page_name,)
        )
        existing = cursor.fetchone()
        
        if existing:
            cursor.close()
            conn.close()
            bot_logger.info(f"âš ï¸ Page '{page_name}' Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± ØµÙ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ (ID: {existing['id']}, Status: {existing['status']})")
            return {
                "status": "info",
                "message": f"Page '{page_name}' Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± ØµÙ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯",
                "queue_id": existing['id'],
                "queue_status": existing['status']
            }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØµÙ Ø¨Ø§ status = pending
        cursor.execute(
            "INSERT INTO pages_queue (page_name, user_id, status) VALUES (%s, %s, 'pending')",
            (page_name, user_id)
        )
        conn.commit()
        queue_id = cursor.lastrowid
        cursor.close()
        conn.close()
        
        bot_logger.info(f"âœ… Page '{page_name}' Ø¨Ø§ ID {queue_id} Ø¯Ø± ØµÙ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª - Worker Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯")
        
        return {
            "status": "success",
            "message": f"Page '{page_name}' Ø¯Ø± ØµÙ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª Ùˆ Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ ØªÙˆØ³Ø· Worker Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯",
            "queue_id": queue_id,
            "data": {
                "page_name": page_name,
                "user_id": user_id
            }
        }
        
    except Exception as e:
        bot_logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Page Ø¨Ù‡ ØµÙ: {e}")
        return {"status": "error", "message": str(e)}, 500


def get_page2(page_name, page_queue_id=None, user_id=None):
    """
    Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ø§Ù…Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª LinkedIn Company Page
    - Ù…Ø±Ø­Ù„Ù‡ 1: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ
    - Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØµÙØ­Ù‡ /about
    Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ÙÙ‚Ø· ØªÙˆØ³Ø· Worker ØµØ¯Ø§ Ø²Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    
    Args:
        page_name: Ù†Ø§Ù… ØµÙØ­Ù‡ Ø´Ø±Ú©Øª Ø¯Ø± LinkedIn
        page_queue_id: ID ØµÙØ­Ù‡ Ø¯Ø± pages_queue
        user_id: ID Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØª (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        
    Returns:
        dict: Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡ ØµÙØ­Ù‡
    """
    try:
        # Read LinkedIn credentials from environment variables
        random_username = os.getenv("LINKEDIN_USERNAME", "pr1")
        random_password = os.getenv("LINKEDIN_PASSWORD", "")
        
        bot_logger.info(f"ğŸ¢ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØµÙØ­Ù‡ '{page_name}' (Queue ID: {page_queue_id})")
        bot_logger.info(f"ğŸ” Using LinkedIn account: {random_username}")
        
        # ØªÙ†Ø¸ÛŒÙ… Socket Handler Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØª
        socket_handler = get_socket_handlers()
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² bot Ø³Ø±Ø§Ø³Ø±ÛŒ
        bot = get_global_bot(username=random_username, password=random_password, user_id=5, is_first=1)
        
        # ===== Ù…Ø±Ø­Ù„Ù‡ 1: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ =====
        url = f"https://www.linkedin.com/company/{page_name}/"
        bot_logger.info(f"ğŸŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ: {url}")
        
        if user_id:
            socket_handler.emit_progress(
                event='page_scraping_started',
                data={"message": f"Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØµÙØ­Ù‡ {page_name}", "url": url},
                user_id=user_id
            )
        
        bot.driver.get(url)
        bot_logger.info("âœ… Page loaded, sleeping...")
        time.sleep(10)
        bot_logger.info("âœ… Sleep done, starting scroll...")
        
        # Scroll down to load content
        try:
            bot.driver.execute_script("window.scrollTo(0, 800);")
            time.sleep(3)
            bot.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
        except:
            pass
        
        # Debug: Log page source and check for authwall
        bot_logger.info("ğŸ” Starting page source check...")
        try:
            page_source = bot.driver.page_source
            page_source_len = len(page_source)
            bot_logger.info(f"ğŸ“„ Ø·ÙˆÙ„ HTML ØµÙØ­Ù‡: {page_source_len} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            
            # Check if redirected to login or authwall
            current_url = bot.driver.current_url
            bot_logger.info(f"ğŸŒ Current URL: {current_url}")
            if 'login' in current_url.lower() or 'authwall' in current_url.lower():
                bot_logger.warning(f"âš ï¸ Redirect Ø¨Ù‡ ØµÙØ­Ù‡ login! URL: {current_url}")
            
            # Check for authwall in page source
            if 'authwall' in page_source.lower() or 'join now' in page_source.lower():
                bot_logger.warning("âš ï¸ LinkedIn authwall detected! Trying to login...")
                try:
                    bot.login(username=random_username, password="")
                    time.sleep(5)
                    bot.driver.get(url)
                    time.sleep(10)
                    bot_logger.info("âœ… Re-loaded page after login attempt")
                except Exception as login_error:
                    bot_logger.error(f"âŒ Login failed: {login_error}")
            
            # Log some page indicators
            if 'org-top-card' in page_source:
                bot_logger.info("âœ… org-top-card detected in page")
            else:
                bot_logger.warning("âš ï¸ org-top-card NOT found in page!")
                
        except Exception as e:
            bot_logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ ØµÙØ­Ù‡: {e}", exc_info=True)
        
        # Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        page_title = page_name
        page_description = ""
        page_overview = ""
        last_post_content = ""
        company_industry = ""
        company_location = ""
        company_followers = ""
        company_employees = ""
        company_link = ""
        company_phone = ""
        company_value = ""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ø´Ø±Ú©Øª (page_title) - Ø¨Ø§ Ú†Ù†Ø¯ÛŒÙ† Ø±ÙˆØ´ fallback
        try:
            # Ø±ÙˆØ´ 1: XPath Ù‚Ø¯ÛŒÙ…ÛŒ
            try:
                page_title = bot.driver.find_element(By.XPATH, '//h1[contains(@class, "org-top-card-summary__title")]').text
            except:
                # Ø±ÙˆØ´ 2: Ù‡Ø± h1 Ø¯Ø± top card
                try:
                    page_title = bot.driver.find_element(By.XPATH, '//div[contains(@class, "org-top-card")]//h1').text
                except:
                    # Ø±ÙˆØ´ 3: h1 Ø¨Ø§ Ú©Ù„Ø§Ø³ org-
                    try:
                        page_title = bot.driver.find_element(By.XPATH, '//h1[contains(@class, "org-")]').text
                    except:
                        # Ø±ÙˆØ´ 4: Ø§ÙˆÙ„ÛŒÙ† h1 Ø¯Ø± ØµÙØ­Ù‡
                        page_title = bot.driver.find_element(By.TAG_NAME, 'h1').text
            
            page_title = page_title.strip()
            bot_logger.info(f"âœ… Ù†Ø§Ù… Ø´Ø±Ú©Øª: {page_title}")
            
            if user_id:
                socket_handler.emit_progress(
                    event='company_name',
                    data={"message": "Ù†Ø§Ù… Ø´Ø±Ú©Øª", "company_name": page_title},
                    user_id=user_id
                )
        except Exception as e:
            bot_logger.warning(f"âš ï¸ Ù†Ø§Ù… Ø´Ø±Ú©Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: {e}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª (page_description)
        try:
            try:
                page_description = bot.driver.find_element(By.XPATH, '//p[@class="org-top-card-summary__tagline"]').text
            except:
                page_description = bot.driver.find_element(By.XPATH, '//div[contains(@class, "org-top-card")]//p[contains(@class, "tagline")]').text
            bot_logger.info(f"âœ… ØªÙˆØ¶ÛŒØ­Ø§Øª: {page_description}")
        except Exception as e:
            bot_logger.warning(f"âš ï¸ ØªÙˆØ¶ÛŒØ­Ø§Øª ÛŒØ§ÙØª Ù†Ø´Ø¯: {e}")
        
        # ===== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø³Øª Ø§Ø² ØµÙØ­Ù‡ Posts =====
        # Ø¨Ù‡ Ø¬Ø§ÛŒ featured post Ø§Ø² main pageØŒ Ø¨Ù‡ /posts Ù…ÛŒâ€ŒØ±ÙˆÛŒÙ…
        posts_url = f"https://www.linkedin.com/company/{page_name}/posts"
        bot_logger.info(f"ğŸŒ Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ Posts: {posts_url}")
        bot.driver.get(posts_url)
        time.sleep(10)
        
        last_post_time = None
        try:
            post_found = False
            
            # Ø±ÙˆØ´ 1: feed-shared-update-v2 (structure Ø¬Ø¯ÛŒØ¯) - Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø³Øª = Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
            try:
                posts = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "feed-shared-update-v2")]')
                bot_logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø³Øªâ€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(posts)}")
                if posts and len(posts) > 0:
                    # Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø³Øª = Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù¾Ø³Øª Ø¨Ù‡ ØµÙˆØ±Øª Ø²Ù…Ø§Ù†ÛŒ
                    post = posts[0]
                    try:
                        # Try feed-shared-text__text-view
                        text_elem = post.find_element(By.XPATH, './/div[contains(@class, "feed-shared-text")]//span[@dir="ltr"]')
                        last_post_content = text_elem.text.strip()
                        post_found = True
                    except:
                        try:
                            # Try update-components-text
                            text_elem = post.find_element(By.XPATH, './/div[contains(@class, "update-components-text")]')
                            last_post_content = text_elem.text.strip()
                            post_found = True
                        except:
                            pass
            except:
                pass
            
            # Ø±ÙˆØ´ 2: Any span with dir=ltr in posts section
            if not post_found:
                try:
                    all_spans = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "scaffold-finite-scroll")]//span[@dir="ltr"]')
                    if len(all_spans) > 0:
                        for span in all_spans:
                            text = span.text.strip()
                            if len(text) > 20:  # At least 20 chars to be meaningful
                                last_post_content = text
                                post_found = True
                                break
                except:
                    pass
            
            # Ø±ÙˆØ´ 3: feed-shared-inline-show-more-text
            if not post_found:
                try:
                    text_elem = bot.driver.find_element(By.XPATH, '//div[contains(@class, "feed-shared-inline-show-more-text")]')
                    last_post_content = text_elem.text.strip()
                    post_found = True
                except:
                    pass
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§
            comments_count = 0
            comments_text = ""
            if post_found:
                try:
                    # Try to find comment count
                    comment_count_elements = bot.driver.find_elements(By.XPATH, '//button[contains(@aria-label, "comment")]//span[contains(@class, "social-details-social-counts__reactions-count")]')
                    if not comment_count_elements:
                        comment_count_elements = bot.driver.find_elements(By.XPATH, '//button[contains(., "comment")]//span[@aria-hidden="true"]')
                    
                    for elem in comment_count_elements:
                        text = elem.text.strip()
                        if text and any(char.isdigit() for char in text):
                            # Extract number from text like "23 comments" or "5"
                            import re
                            match = re.search(r'(\d+)', text)
                            if match:
                                comments_count = int(match.group(1))
                                bot_logger.info(f"ğŸ’¬ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§: {comments_count}")
                                break
                    
                    # Extract comment texts
                    if comments_count > 0:
                        # Scroll to load comments
                        try:
                            bot.driver.execute_script("window.scrollTo(0, 600);")
                            time.sleep(2)
                        except:
                            pass
                        
                        comment_elements = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "comments-comment-item")]//span[@dir="ltr"]')
                        if not comment_elements:
                            comment_elements = bot.driver.find_elements(By.XPATH, '//article[contains(@class, "comments-comment")]//span[@dir="ltr"]')
                        
                        comments_list = []
                        for comment_elem in comment_elements[:10]:  # Get first 10 comments
                            comment_text = comment_elem.text.strip()
                            if comment_text and len(comment_text) > 5:
                                comments_list.append(comment_text)
                        
                        if comments_list:
                            comments_text = " | ".join(comments_list)
                            bot_logger.info(f"âœ… {len(comments_list)} Ú©Ø§Ù…Ù†Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
                        else:
                            bot_logger.info("âš ï¸ Ù…ØªÙ† Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                except Exception as e:
                    bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù†Øªâ€ŒÙ‡Ø§: {e}")
            
            if post_found and last_post_content:
                bot_logger.info(f"âœ… Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø³Øª ({len(last_post_content)} Ú©Ø§Ø±Ø§Ú©ØªØ±): {last_post_content[:60]}...")
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ù¾Ø³Øª - Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ù¾Ø³Øª Ø¯Ø± /posts
                try:
                    from dateutil import parser
                    time_extracted = False
                    
                    # Ø±ÙˆØ´ 1: time with datetime attribute anywhere in page
                    try:
                        time_elements = bot.driver.find_elements(By.XPATH, '//time[@datetime]')
                        bot_logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ time elements Ø¨Ø§ datetime: {len(time_elements)}")
                        if time_elements:
                            datetime_str = time_elements[0].get_attribute('datetime')
                            bot_logger.info(f"ğŸ• datetime string: {datetime_str}")
                            dt = parser.parse(datetime_str)
                            last_post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                            bot_logger.info(f"âœ… Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ù¾Ø³Øª (Ø±ÙˆØ´ 1): {last_post_time}")
                            time_extracted = True
                    except Exception as e1:
                        bot_logger.info(f"Ø±ÙˆØ´ 1 Ù†Ø§Ù…ÙˆÙÙ‚: {e1}")
                    
                    # Ø±ÙˆØ´ 2: any time tag
                    if not time_extracted:
                        try:
                            time_elements = bot.driver.find_elements(By.TAG_NAME, 'time')
                            bot_logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ time elements: {len(time_elements)}")
                            if time_elements:
                                # Try each time element
                                for i, time_elem in enumerate(time_elements[:5]):
                                    datetime_str = time_elem.get_attribute('datetime')
                                    if datetime_str:
                                        dt = parser.parse(datetime_str)
                                        last_post_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                                        bot_logger.info(f"âœ… Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ù¾Ø³Øª (Ø±ÙˆØ´ 2, element {i}): {last_post_time}")
                                        time_extracted = True
                                        break
                        except Exception as e2:
                            bot_logger.info(f"Ø±ÙˆØ´ 2 Ù†Ø§Ù…ÙˆÙÙ‚: {e2}")
                    
                    # Ø±ÙˆØ´ 3: text like "1d", "2w", etc - relative time
                    if not time_extracted:
                        try:
                            time_texts = bot.driver.find_elements(By.XPATH, '//*[contains(@class, "update-components-actor__sub-description")]//span')
                            bot_logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ time text elements: {len(time_texts)}")
                            if time_texts:
                                time_text = time_texts[0].text.strip()
                                bot_logger.info(f"â° Ø²Ù…Ø§Ù† Ù†Ø³Ø¨ÛŒ: {time_text}")
                                
                                # Parse relative time (4d, 2w, 3mo, etc)
                                import datetime
                                import re
                                now = datetime.datetime.now()
                                
                                # Extract number and unit
                                match = re.search(r'(\d+)([a-z]+)', time_text.lower())
                                if match:
                                    value = int(match.group(1))
                                    unit = match.group(2)
                                    
                                    if unit in ['d', 'day', 'days']:
                                        post_time = now - datetime.timedelta(days=value)
                                    elif unit in ['w', 'wk', 'week', 'weeks']:
                                        post_time = now - datetime.timedelta(weeks=value)
                                    elif unit in ['h', 'hr', 'hour', 'hours']:
                                        post_time = now - datetime.timedelta(hours=value)
                                    elif unit in ['m', 'min', 'minute', 'minutes']:
                                        post_time = now - datetime.timedelta(minutes=value)
                                    elif unit in ['mo', 'month', 'months']:
                                        post_time = now - datetime.timedelta(days=value*30)
                                    elif unit in ['y', 'yr', 'year', 'years']:
                                        post_time = now - datetime.timedelta(days=value*365)
                                    else:
                                        post_time = now
                                    
                                    last_post_time = post_time.strftime('%Y-%m-%d %H:%M:%S')
                                    bot_logger.info(f"âœ… Ø²Ù…Ø§Ù† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø§Ø² '{time_text}': {last_post_time}")
                                    time_extracted = True
                                else:
                                    bot_logger.warning(f"âš ï¸ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… '{time_text}' Ø±Ø§ parse Ú©Ù†Ù…")
                        except Exception as e3:
                            bot_logger.info(f"Ø±ÙˆØ´ 3 Ù†Ø§Ù…ÙˆÙÙ‚: {e3}")
                    
                    if not time_extracted:
                        bot_logger.warning("âš ï¸ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ù¾Ø³Øª ÛŒØ§ÙØª Ù†Ø´Ø¯ - Ø§Ø² Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§")
                except Exception as time_error:
                    bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù†: {time_error}")
                    import traceback
                    bot_logger.warning(traceback.format_exc())
            else:
                bot_logger.warning("âš ï¸ Ù…Ø­ØªÙˆØ§ÛŒ Ù¾Ø³Øª ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª")
        except Exception as e:
            bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø³Øª: {e}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª - Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ù‚ÛŒÙ‚ HTML LinkedIn
        bot_logger.info("ğŸ“Š Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª...")
        try:
            # Industry (ØµÙ†Ø¹Øª) - Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
            try:
                industry_found = False
                # Ø±ÙˆØ´ 1: org-top-card-summary-info-list structure
                try:
                    industry_elem = bot.driver.find_element(By.XPATH, '//div[contains(@class, "org-top-card-summary-info-list")]//div[contains(@class, "org-page-details__definition-text")]')
                    company_industry = industry_elem.text.strip()
                    industry_found = True
                except:
                    pass
                
                # Ø±ÙˆØ´ 2: Find by "Industry" text
                if not industry_found:
                    try:
                        dt_elements = bot.driver.find_elements(By.XPATH, '//dt[contains(., "Industry")]/following-sibling::dd')
                        if dt_elements:
                            company_industry = dt_elements[0].text.strip()
                            industry_found = True
                    except:
                        pass
                
                # Ø±ÙˆØ´ 3: Any text in org-top-card that looks like industry
                if not industry_found:
                    try:
                        all_text_elements = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "org-top-card")]//div[contains(@class, "t-black--light")]')
                        for elem in all_text_elements:
                            text = elem.text.strip()
                            # Industry text usually doesn't contain numbers or commas
                            if text and not any(char in text for char in [',', 'follower', 'employee']) and len(text) > 5:
                                company_industry = text
                                industry_found = True
                                break
                    except:
                        pass
                
                if industry_found:
                    bot_logger.info(f"âœ… ØµÙ†Ø¹Øª (Industry): '{company_industry}'")
                else:
                    bot_logger.warning("âš ï¸ ØµÙ†Ø¹Øª ÛŒØ§ÙØª Ù†Ø´Ø¯")
            except Exception as e:
                bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙ†Ø¹Øª: {e}")
            
            # Location, Followers, Employees - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹
            try:
                # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø§Ø² top card
                all_info_texts = []
                
                # Ø±ÙˆØ´ 1: org-top-card-summary-info-list__info-item
                try:
                    items = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "org-top-card-summary-info-list__info-item")]')
                    all_info_texts.extend([item.text.strip() for item in items if item.text.strip()])
                except:
                    pass
                
                # Ø±ÙˆØ´ 2: org-top-card-summary__info-item
                try:
                    items = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "org-top-card-summary__info-item")]')
                    all_info_texts.extend([item.text.strip() for item in items if item.text.strip()])
                except:
                    pass
                
                # Ø±ÙˆØ´ 3: Any div with company stats in org-top-card
                try:
                    items = bot.driver.find_elements(By.XPATH, '//div[contains(@class, "org-top-card")]//div[contains(@class, "t-black--light") or contains(@class, "org-top-card-summary-info-list")]')
                    all_info_texts.extend([item.text.strip() for item in items if item.text.strip() and len(item.text.strip()) > 3])
                except:
                    pass
                
                # Remove duplicates while preserving order
                seen = set()
                unique_texts = []
                for text in all_info_texts:
                    if text and text not in seen:
                        seen.add(text)
                        unique_texts.append(text)
                
                bot_logger.info(f"âœ… Ù¾ÛŒØ¯Ø§ Ø´Ø¯ {len(unique_texts)} Ø¢ÛŒØªÙ… Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯")
                
                # ØªØ­Ù„ÛŒÙ„ Ùˆ ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ù‡Ø± Ø¢ÛŒØªÙ…
                for text in unique_texts:
                    text_lower = text.lower()
                    
                    # ØªØ´Ø®ÛŒØµ Location (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„ ÙˆÛŒØ±Ú¯ÙˆÙ„ Ùˆ Ù†Ø§Ù… Ø´Ù‡Ø±/Ú©Ø´ÙˆØ±)
                    if not company_location:
                        if ',' in text and not any(keyword in text_lower for keyword in ['follower', 'employee', 'on linkedin']):
                            company_location = text
                            bot_logger.info(f"âœ… Ù…ÙˆÙ‚Ø¹ÛŒØª (Location): '{company_location}'")
                    
                    # ØªØ´Ø®ÛŒØµ Followers (Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ù‡ follower)
                    if not company_followers:
                        if 'follower' in text_lower:
                            company_followers = text
                            bot_logger.info(f"âœ… ÙØ§Ù„ÙˆØ¦Ø±Ù‡Ø§ (Followers): '{company_followers}'")
                            if user_id:
                                socket_handler.emit_progress(
                                    event='followers_count',
                                    data={"message": "ØªØ¹Ø¯Ø§Ø¯ ÙØ§Ù„ÙˆØ¦Ø±", "followers": company_followers},
                                    user_id=user_id
                                )
                            continue
                    
                    # ØªØ´Ø®ÛŒØµ Employees (Ø´Ø§Ù…Ù„ employee ÛŒØ§ on LinkedIn)
                    if not company_employees:
                        if 'employee' in text_lower or 'on linkedin' in text_lower:
                            company_employees = text
                            bot_logger.info(f"âœ… Ú©Ø§Ø±Ù…Ù†Ø¯Ø§Ù† (Employees): '{company_employees}'")
                            continue
                    
            except Exception as e:
                bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª: {e}")
                
                # Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ…Ø§Ù… info-items
                try:
                    all_items = bot.driver.find_elements(
                        By.XPATH,
                        '//div[@class="org-top-card-summary-info-list"]//div[@class="org-top-card-summary-info-list__info-item"]'
                    )
                    
                    bot_logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ info-items: {len(all_items)}")
                    
                    # Industry = Ø§ÙˆÙ„ÛŒÙ†ØŒ Location = Ø¯ÙˆÙ…ÛŒÙ†ØŒ Followers = Ø³ÙˆÙ…ÛŒÙ†
                    if len(all_items) >= 2:
                        company_location = all_items[1].text.strip()
                        bot_logger.info(f"âœ… Ù…ÙˆÙ‚Ø¹ÛŒØª (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†): '{company_location}'")
                    
                    if len(all_items) >= 3:
                        followers_text = all_items[2].text.strip()
                        company_followers = ' '.join(followers_text.split())
                        bot_logger.info(f"âœ… ÙØ§Ù„ÙˆØ¦Ø±Ù‡Ø§ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†): '{company_followers}'")
                        
                except Exception as e:
                    bot_logger.error(f"âš ï¸ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
                
                # Employees Ø§Ø² Ù„ÛŒÙ†Ú©
                try:
                    employees_elem = bot.driver.find_element(
                        By.XPATH,
                        '//div[@class="org-top-card-summary-info-list"]//a[contains(@href, "/search/results/people/")]//span'
                    )
                    employees_text = employees_elem.text.strip()
                    company_employees = ' '.join(employees_text.split())
                    bot_logger.info(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ù…Ù†Ø¯Ø§Ù† (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†): '{company_employees}'")
                except Exception as e:
                    bot_logger.warning(f"âš ï¸ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ù…Ù†Ø¯Ø§Ù† Ø¨Ø§ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† ÛŒØ§ÙØª Ù†Ø´Ø¯: {e}")
                
        except Exception as e:
            bot_logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª: {e}")
            import traceback
            bot_logger.error(traceback.format_exc())
        
        # ===== Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø² ØµÙØ­Ù‡ About =====
        about_url = f"https://www.linkedin.com/company/{page_name}/about"
        bot_logger.info(f"ğŸŒ Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡ About: {about_url}")
        bot.driver.get(about_url)
        time.sleep(10)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ù‡ ØµÙØ­Ù‡ login redirect Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´ÛŒÙ…
        current_url = bot.driver.current_url
        if "login" in current_url.lower() or "signin" in current_url.lower():
            bot_logger.warning(f"âš ï¸ Redirect Ø¨Ù‡ login - ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...")
            bot.login(username=random_username, password="")
            time.sleep(5)
            bot.driver.get(about_url)
            time.sleep(10)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙ†Ø¹Øª (company_industry) Ø§Ø² About page - Ø¨Ù‡ØªØ± Ø§Ø² main page
        try:
            industry_found = False
            
            # Debug: log all dt elements
            try:
                all_dts = bot.driver.find_elements(By.XPATH, '//dt')
                bot_logger.info(f"ğŸ“‹ ØªØ¹Ø¯Ø§Ø¯ dt elements: {len(all_dts)}")
                for i, dt in enumerate(all_dts[:10]):  # First 10
                    bot_logger.info(f"  dt[{i}]: {dt.text[:50] if dt.text else 'empty'}")
            except:
                pass
            
            # Ø±ÙˆØ´ 1: dt with "Industry" text
            try:
                industry_element = bot.driver.find_element(By.XPATH, '//dt[contains(text(), "Industry")]/following-sibling::dd')
                company_industry = industry_element.text.strip()
                if company_industry:
                    industry_found = True
                    bot_logger.info(f"âœ… ØµÙ†Ø¹Øª (Ø±ÙˆØ´ 1): {company_industry}")
            except Exception as e:
                bot_logger.info(f"Ø±ÙˆØ´ 1 ØµÙ†Ø¹Øª Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
            
            # Ø±ÙˆØ´ 2: case-insensitive search
            if not industry_found:
                try:
                    dts = bot.driver.find_elements(By.XPATH, '//dt')
                    bot_logger.info(f"ğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± {len(dts)} dt element...")
                    for i, dt in enumerate(dts):
                        dt_text = dt.text.strip().lower()
                        bot_logger.info(f"  Ø¨Ø±Ø±Ø³ÛŒ dt[{i}]: '{dt_text}'")
                        if 'industry' in dt_text:
                            dd = dt.find_element(By.XPATH, './following-sibling::dd[1]')
                            company_industry = dd.text.strip()
                            if company_industry:
                                industry_found = True
                                bot_logger.info(f"âœ… ØµÙ†Ø¹Øª (Ø±ÙˆØ´ 2): {company_industry}")
                                break
                except Exception as e2:
                    bot_logger.info(f"Ø±ÙˆØ´ 2 ØµÙ†Ø¹Øª Ù†Ø§Ù…ÙˆÙÙ‚: {e2}")
            
            # Ø±ÙˆØ´ 3: xpath with normalize-space
            if not industry_found:
                try:
                    industry_element = bot.driver.find_element(By.XPATH, '//dt[normalize-space(translate(., "INDUSTRY", "industry"))="industry"]/following-sibling::dd')
                    company_industry = industry_element.text.strip()
                    if company_industry:
                        industry_found = True
                        bot_logger.info(f"âœ… ØµÙ†Ø¹Øª (Ø±ÙˆØ´ 3): {company_industry}")
                except Exception as e3:
                    bot_logger.info(f"Ø±ÙˆØ´ 3 ØµÙ†Ø¹Øª Ù†Ø§Ù…ÙˆÙÙ‚: {e3}")
            
            if not industry_found:
                bot_logger.warning("âš ï¸ ØµÙ†Ø¹Øª Ø¯Ø± About page ÛŒØ§ÙØª Ù†Ø´Ø¯")
        except Exception as e:
            bot_logger.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙ†Ø¹Øª Ø§Ø² About: {e}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ¨Ø³Ø§ÛŒØª (company_link) - Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        try:
            website_found = False
            # Ø±ÙˆØ´ 1: dt/dd structure
            try:
                website_element = bot.driver.find_element(By.XPATH, '//dt[contains(., "Website")]/following-sibling::dd//a[@href]')
                company_link = website_element.get_attribute('href')
                website_found = True
            except:
                # Ø±ÙˆØ´ 2: Search for any external link in about section
                try:
                    links = bot.driver.find_elements(By.XPATH, '//section[contains(@class, "about")]//a[@href]')
                    for link in links:
                        href = link.get_attribute('href')
                        if href and 'linkedin.com' not in href and href.startswith('http'):
                            company_link = href
                            website_found = True
                            break
                except:
                    pass
            
            if website_found:
                bot_logger.info(f"âœ… ÙˆØ¨Ø³Ø§ÛŒØª: {company_link}")
            
            if user_id:
                socket_handler.emit_progress(
                    event='company_website',
                    data={"message": "ÙˆØ¨Ø³Ø§ÛŒØª Ø´Ø±Ú©Øª", "website": company_link},
                    user_id=user_id
                )
        except Exception:
            bot_logger.warning("âš ï¸ ÙˆØ¨Ø³Ø§ÛŒØª ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† (company_phone)
        try:
            phone_element = bot.driver.find_element(By.XPATH, '//dt[contains(., "Phone")]/following-sibling::dd//a[@href]')
            company_phone = phone_element.get_attribute('href').replace('tel:', '')
            bot_logger.info(f"âœ… ØªÙ„ÙÙ†: {company_phone}")
        except Exception:
            bot_logger.warning("âš ï¸ ØªÙ„ÙÙ† ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø§Ù„ ØªØ§Ø³ÛŒØ³ (company_value)
        try:
            founded_element = bot.driver.find_element(By.XPATH, '//dt[contains(., "Founded")]/following-sibling::dd')
            company_value = founded_element.text.strip()
            bot_logger.info(f"âœ… Ø³Ø§Ù„ ØªØ§Ø³ÛŒØ³: {company_value}")
        except Exception:
            bot_logger.warning("âš ï¸ Ø³Ø§Ù„ ØªØ§Ø³ÛŒØ³ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            # Fallback
            try:
                value_elements = bot.driver.find_elements(By.XPATH, '//dd[@class="mb4 t-black--light text-body-medium"]')
                if value_elements:
                    company_value = value_elements[-1].text
            except Exception:
                pass
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Overview (page_overview) - Ø¨Ø§ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        try:
            overview_found = False
            # Ø±ÙˆØ´ 1: h2 + sibling p
            try:
                page_overview = bot.driver.find_element(By.XPATH, '//h2[contains(., "Overview")]/following-sibling::p').text
                overview_found = True
            except:
                # Ø±ÙˆØ´ 2: div with overview class
                try:
                    page_overview = bot.driver.find_element(By.XPATH, '//div[contains(@class, "overview")]//p').text
                    overview_found = True
                except:
                    # Ø±ÙˆØ´ 3: section about with first paragraph
                    try:
                        page_overview = bot.driver.find_element(By.XPATH, '//section[contains(@class, "about")]//p').text
                        overview_found = True
                    except:
                        pass
            
            if overview_found:
                bot_logger.info(f"âœ… Overview: {page_overview[:50]}...")
        except Exception:
            bot_logger.warning("âš ï¸ Overview ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        # ===== Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ =====
        if page_title:  # ÙÙ‚Ø· Ø§Ú¯Ø± Ø­Ø¯Ø§Ù‚Ù„ Ù†Ø§Ù… Ø´Ø±Ú©Øª Ø¯Ø§Ø´ØªÛŒÙ…
            conn = get_db_connection()
            cursor = conn.cursor()
            
            bot_logger.info("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
            cursor.execute(
                """
                INSERT INTO linkdeen_pages (
                    page_title,
                    page_description,
                    page_overview,
                    last_post_content,
                    post_created_at,
                    company_industry,
                    company_location,
                    company_followers,
                    company_employees,
                    company_link,
                    company_phone,
                    company_value,
                    comments,
                    comments_text,
                    page_queue_id
                ) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
                """,
                (
                    page_title,
                    page_description,
                    page_overview,
                    last_post_content,
                    last_post_time,
                    company_industry,
                    company_location,
                    company_followers,
                    company_employees,
                    company_link,
                    company_phone,
                    company_value,
                    comments_count,
                    comments_text,
                    page_queue_id,
                ),
            )
            conn.commit()
            page_id = cursor.lastrowid
            cursor.close()
            conn.close()
            
            bot_logger.info(f"âœ… ØµÙØ­Ù‡ '{page_name}' Ø¨Ø§ ID {page_id} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
            
            if user_id:
                socket_handler.emit_progress(
                    event='page_saved',
                    data={"message": "ØµÙØ­Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯", "page_id": page_id},
                    user_id=user_id
                )
        
        # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        return {
            'page_title': page_title,
            'page_description': page_description,
            'page_overview': page_overview,
            'last_post_content': last_post_content,
            'post_created_at': last_post_time,
            'company_info': {
                'industry': company_industry,
                'location': company_location,
                'followers': company_followers,
                'employees': company_employees
            },
            'company_link': company_link,
            'company_phone': company_phone,
            'company_value': company_value,
            'members_count': ""
        }
        
    except Exception as e:
        bot_logger.error(f"âŒ Error in get_page2: {str(e)}")
        raise  # Ù…Ù‡Ù…: Ø®Ø·Ø§ Ø±Ùˆ Ø¨Ø§Ù„Ø§ Ø¨ÙØ±Ø³Øª ØªØ§ Worker Ø¨Ú¯ÛŒØ±ØªØ´


def add_account(request):
    user = request.user
    data = request.get_json()
    user_id = user.id
    username = data.get("username")
    password = data.get("password")
    bot = None
    if not username or not password:
        return {"status": "error", "message": "Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ùˆ Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø§Ù„Ø²Ø§Ù…ÛŒ Ù‡Ø³ØªÙ†Ø¯"}, 400

    try:
        
        conn = get_db_connection()
        cursor = conn.cursor()
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù¾ÛŒØ¬
        cursor.execute("SELECT username FROM pages WHERE username = %s AND user_id = %s", 
                      (username, user.id))
        existing_page = cursor.fetchone()
        
        if existing_page:
            
            page_id = existing_page['id']
            bot_logger.info(f"Ù¾ÛŒØ¬ {username} Ù‚Ø¨Ù„Ø§Ù‹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù‡ÛŒÚ† Ø§Ù‚Ø¯Ø§Ù…ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯")
            time.sleep(20)
            if bot:
                bot.cleanup()
            conn.close()
            return {
                "status": "success",
                "message": f"Ù¾ÛŒØ¬ '{username}' Ù‚Ø¨Ù„Ø§Ù‹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯"
            }, 200
        else:
            
            cursor.execute("SELECT username FROM pages WHERE username = %s", (username,))
            exist_page = cursor.fetchone()
            if exist_page:
                bot_logger.info("Ù¾ÛŒØ¬ Ù‚Ø¨Ù„Ø§ Ø¨Ø§ Ø§Ú©Ø§Ù†Øª Ú©Ø³ Ø¯ÛŒÚ¯Ø±ÛŒ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª")
                conn.close()
                return {
                    "status": "error",
                    "message": f"Ù¾ÛŒØ¬ {username} Ø¨Ø±Ø§ÛŒ Ú©Ø³ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø³Øª"
                }, 403

            bot = LinkedinBot(username, is_first=1)
            
            #bot.login(username, password, user_id)

            bot_logger.info("Ù¾ÛŒØ¬ Ø´Ù…Ø§ ØªØ§Ø²Ù‡ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª Ø¨Ø§ÛŒØ¯ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ ÙØ§Ù„ÙˆØ¦Ø±Ù‡Ø§ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…")
            # Ø¯Ø±Ø¬ Ù¾ÛŒØ¬ Ø¬Ø¯ÛŒØ¯
            
            cursor.execute("""
                INSERT INTO pages (username, user_id)
                VALUES (%s, %s)
            """, (username, user.id))
            page_id = cursor.lastrowid
            
            conn.commit()

        

            
            if bot:
                pass
                #bot.cleanup()

            conn.close()
            return {
                "status": "success",
                "message": f"Ú©Ø§Ø±Ø¨Ø± '{username}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯ Ùˆ ÙØ§Ù„ÙˆØ¦Ø±Ù‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯",
                "data": {"page_id": page_id, "username": username}
            }, 201

    except LoginError as e:
        bot_logger.error(f"Ø®Ø·Ø§ÛŒ ÙˆØ±ÙˆØ¯ Ø¨Ø±Ø§ÛŒ {username}: {str(e)}")
        if bot:
            pass
            #bot.cleanup()
        return {"status": "error", "message": f"Ø®Ø·Ø§ÛŒ ÙˆØ±ÙˆØ¯: {str(e)}"}, 401
    except pymysql.MySQLError as e:
        bot_logger.error(f"Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ {username}: {str(e)}")
        if bot:
            pass
            #bot.cleanup()
        return {"status": "error", "message": f"Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}"}, 500
    except Exception as e:
        bot_logger.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¨Ø±Ø§ÛŒ {username}: {str(e)}")
        if bot:
            pass
            #bot.cleanup()
        return {"status": "error", "message": f"Ø®Ø·Ø§: {str(e)}"}, 500

def list_pages(request):
    user = request.user
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT id, username, followers, following, caption, number_of_posts, profile_picture_path
            FROM pages 
            WHERE user_id = %s
        """, (user.id,))
        pages = cursor.fetchall()
        conn.close()
        print(pages)

        if not pages:
            return {"status": "success", "message": "Ù‡ÛŒÚ† Ù¾ÛŒØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯", "data": []}, 200
        
        page_list = []
        for page in pages:
            profile_picture_base64 = None
            if page['profile_picture_path']:
                try:
                    with open(page['profile_picture_path'], 'rb') as image_file:
                        profile_picture_base64 = base64.b64encode(image_file.read()).decode('utf-8')
                except Exception as e:
                    bot_logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ ØªØµÙˆÛŒØ± Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ø¨Ù‡ Base64 Ø¨Ø±Ø§ÛŒ {page['username']}: {str(e)}")

            page_list.append({
                "id": page['id'],
                "username": page['username'],
                "followers": page['followers'],
                "following": page['following'],
                "caption": page['caption'],
                "number_of_posts": page['number_of_posts'],
                "profile_picture_base64": profile_picture_base64
            })

        bot_logger.info(f"{len(page_list)} Ù¾ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± {user.phone_number} Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯")
        return {"status": "success", "message": "Ù¾ÛŒØ¬â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø´Ø¯Ù†Ø¯", "data": page_list}, 200

    except pymysql.MySQLError as e:
        bot_logger.error(f"Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ø§ÛŒ {user.phone_number}: {str(e)}")
        return {"status": "error", "message": f"Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {str(e)}"}, 500
    except Exception as e:
        bot_logger.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¨Ø±Ø§ÛŒ {user.phone_number}: {str(e)}")
        return {"status": "error", "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù¾ÛŒØ¬â€ŒÙ‡Ø§: {str(e)}"}, 500
